{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reddit Post Cleaning for BERT Uncased Model\n",
    "\n",
    "This notebook demonstrates how to clean Reddit posts for use with BERT uncased models. The preprocessing steps are designed to prepare text data for generating high-quality embeddings."
   ],
   "id": "5ee5a24c81687694"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For demonstration purposes, we'll create sample Reddit posts\n",
    "sample_posts = [\n",
    "    \"Check out this cool link: https://www.reddit.com/r/MachineLearning and subscribe r/datascience\",\n",
    "    \"I've been using BERT for NLP tasks - it's amazing! #NLP #MachineLearning\",\n",
    "    \"Question about PyTorch vs TensorFlow? Which one should I use for my project?\",\n",
    "    \"Just released my new project on GitHub: github.com/user/project\",\n",
    "    \"This post has some *formatting* and **bold text** with [links](https://example.com)\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'post_text': sample_posts})\n",
    "print(\"Original sample posts:\")\n",
    "for post in sample_posts:\n",
    "    print(f\"- {post}\")"
   ],
   "id": "412fe1aded52fdff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cleaning Functions for BERT Uncased Model\n",
    "\n",
    "BERT uncased models convert all text to lowercase, so our cleaning process needs to:\n",
    "1. Remove URLs and links\n",
    "2. Remove special characters that don't add meaning\n",
    "3. Remove Reddit-specific formatting (markdown, etc.)\n",
    "4. Convert to lowercase (for uncased models)\n",
    "5. Handle whitespace appropri"
   ],
   "id": "b8eae11ac0fb587"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_for_bert_uncased(text):\n",
    "    \"\"\"\n",
    "    Clean text data specifically for BERT uncased model\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text suitable for BERT uncased model\n",
    "    \"\"\"\n",
    "    # Check if text is NaN or None\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to string if not already\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove Reddit-style subreddit references (r/subreddit)\n",
    "    text = re.sub(r'r/\\w+', '', text)\n",
    "\n",
    "    # Remove Reddit-style username mentions (u/username)\n",
    "    text = re.sub(r'u/\\w+', '', text)\n",
    "\n",
    "    # Remove markdown formatting (*, **, [], etc.)\n",
    "    text = re.sub(r'\\*\\*|\\*|\\[|\\]|\\(|\\)|\\_\\_|\\_', '', text)\n",
    "\n",
    "    # Remove hashtags symbol but keep the text\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "\n",
    "    # Remove special characters and punctuation (keep basic punctuation for sentence structure)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-]', '', text)\n",
    "\n",
    "    # Convert to lowercase (for uncased model)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to our sample data\n",
    "df['cleaned_text'] = df['post_text'].apply(clean_for_bert_uncased)\n",
    "\n",
    "# Display the cleaned results\n",
    "print(\"\\nCleaned posts for BERT uncased model:\")\n",
    "for original, cleaned in zip(df['post_text'], df['cleaned_text']):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print(\"-\" * 80"
   ],
   "id": "d8070e8b937633e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a Processing Pipeline for Larger Datasets\n",
    "\n",
    "For larger datasets, you might want to create a more structured pipeline:"
   ],
   "id": "dcbc2c87cb0a0f1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_reddit_posts_for_bert(posts_df, text_column='post_text', batch_size=1000):\n",
    "    \"\"\"\n",
    "    Process a large DataFrame of Reddit posts for BERT uncased model\n",
    "\n",
    "    Args:\n",
    "        posts_df (DataFrame): DataFrame containing Reddit posts\n",
    "        text_column (str): Column name containing the text to clean\n",
    "        batch_size (int): Number of posts to process at once\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with original and cleaned text\n",
    "    \"\"\"\n",
    "    if text_column not in posts_df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in DataFrame\")\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = posts_df.copy()\n",
    "\n",
    "    # Add a column for the cleaned text\n",
    "    result_df['bert_ready_text'] = ''\n",
    "\n",
    "    # Process in batches to handle large datasets\n",
    "    total_rows = len(result_df)\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        end_idx = min(i + batch_size, total_rows)\n",
    "        batch = result_df.iloc[i:end_idx]\n",
    "\n",
    "        # Apply cleaning function\n",
    "        result_df.loc[batch.index, 'bert_ready_text'] = batch[text_column].apply(clean_for_bert_uncased)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed {end_idx}/{total_rows} posts ({end_idx/total_rows*100:.1f}%)\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# For demonstration, let's create a slightly larger dataset\n",
    "larger_sample = sample_posts * 2  # Just duplicate our samples for demonstration\n",
    "larger_df = pd.DataFrame({'post_text': larger_sample})\n",
    "\n",
    "# Process the larger dataset\n",
    "processed_df = process_reddit_posts_for_bert(larger_df, batch_size=5)\n",
    "print(f\"\\nProcessed {len(processed_df)} posts.\")\n",
    "\n",
    "# Show a sample of the processed data\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(processed_df.head(3))"
   ],
   "id": "c9f6c304b5938f18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparing for BERT Embedding Generation\n",
    "\n",
    "After cleaning the text, the next step would be to tokenize it using BERT's tokenizer and generate embeddings. Here's how you would typically do that using the `transformers` library:\n",
    "\n",
    "```python\n",
    "# This code requires the transformers library\n",
    "# pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for a text\n",
    "def get_bert_embedding(text, model, tokenizer):\n",
    "    # Add special tokens and encode\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Tokenize and convert to tensors\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_dict['input_ids'],\n",
    "                        attention_mask=encoded_dict['attention_mask'])\n",
    "\n",
    "    # Use the CLS token embedding as the sentence embedding\n",
    "    sentence_embedding = outputs[0][:, 0, :].numpy()\n",
    "\n",
    "    return sentence_embedding\n",
    "```\n",
    "\n",
    "Note: You would need to install the `transformers` library to run this code. Since it's not in your installed packages list, I've included it as a code example rather than executable code."
   ],
   "id": "7f37a61bd16aeaa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8cb4440dffe6ce4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
